{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzdbH_eFXoEf",
        "outputId": "27fdbb35-f305-46c2-8a6b-c1c8311fe5f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.layers import Lambda, Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "from keras.losses import mse, binary_crossentropy\n",
        "from keras import backend as K\n",
        "\n",
        "from numpy import genfromtxt\n",
        "from numpy import savetxt\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "from mpl_toolkits import mplot3d\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import os\n",
        "from tensorflow.keras.losses import mse, binary_crossentropy\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from google.colab import drive\n",
        "\n",
        "from keras.models import Sequential\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y2SsqHt58qE"
      },
      "outputs": [],
      "source": [
        "# reparameterization trick\n",
        "# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
        "# z = z_mean + sqrt(var) * epsilon\n",
        "def sampling(args):\n",
        "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
        "    # Arguments\n",
        "        args (tensor): mean and log of variance of Q(z|X)\n",
        "    # Returns\n",
        "        z (tensor): sampled latent vector\n",
        "    \"\"\"\n",
        "\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    # by default, random_normal has mean = 0 and std = 1.0\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "\n",
        "def plot_results(models,\n",
        "                 data,\n",
        "                 batch_size=128,\n",
        "                 model_name=\"vae_mnist\"):\n",
        "    \"\"\"Plots labels and MNIST digits as a function of the 2D latent vector\n",
        "    # Arguments\n",
        "        models (tuple): encoder and decoder models\n",
        "        data (tuple): test data and label\n",
        "        batch_size (int): prediction batch size\n",
        "        model_name (string): which model is using this function\n",
        "    \"\"\"\n",
        "\n",
        "    encoder, decoder = models\n",
        "    x_test, y_test = data\n",
        "    os.makedirs(model_name, exist_ok=True)\n",
        "\n",
        "    filename = os.path.join(model_name, \"vae_mean.png\")\n",
        "    # display a 2D plot of the FC classes in the latent space\n",
        "    z_mean, _, _ = encoder.predict(x_test,\n",
        "                                   batch_size=batch_size)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
        "    #savetxt('/content/drive/MyDrive/z_mean0_2D_Midad_Interdim.csv', z_mean[:,0], delimiter=',')\n",
        "    #savetxt('/content/drive/MyDrive/z_mean1_2D_Midad_Interdim.csv', z_mean[:,1], delimiter=',')\n",
        "    #savetxt('/content/drive/MyDrive/labels_dataVAE_2D_Midad_Interdim.csv', y_test, delimiter=',')\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    #plt.xlim(-6,6)\n",
        "    #plt.ylim(-6,6)\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "    filename = os.path.join(model_name, \"FCs_over_latent.png\")\n",
        "    # display a 30x30 2D manifold of FCs\n",
        "    n = 10\n",
        "    digit_size = 115\n",
        "    figure = np.zeros((digit_size * n, digit_size * n))\n",
        "    # linearly spaced coordinates corresponding to the 2D plot\n",
        "    # of FC classes in the latent space\n",
        "    grid_x = np.linspace(-6, 6, n)\n",
        "    grid_y = np.linspace(-6, 6, n)[::-1]\n",
        "\n",
        "    for i, yi in enumerate(grid_y):\n",
        "        for j, xi in enumerate(grid_x):\n",
        "            z_sample = np.array([[xi, yi]])\n",
        "            x_decoded = decoder.predict(z_sample)\n",
        "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "            figure[i * digit_size: (i + 1) * digit_size,\n",
        "                   j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    start_range = digit_size // 2\n",
        "    end_range = (n - 1) * digit_size + start_range + 1\n",
        "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "    sample_range_x = np.round(grid_x, 1)\n",
        "    sample_range_y = np.round(grid_y, 1)\n",
        "    plt.xticks(pixel_range, sample_range_x)\n",
        "    plt.yticks(pixel_range, sample_range_y)\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.imshow(figure) #cmap='Greys_r')\n",
        "    plt.grid(color='w', linewidth=2)\n",
        "    plt.savefig(filename)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75Vhr5OB6DJV"
      },
      "outputs": [],
      "source": [
        "# MNIST dataset\n",
        "\n",
        "# load data\n",
        "my_data = genfromtxt('/content/drive/MyDrive/dataset_FCsim.csv',delimiter=',') # my_data = genfromtxt('FC_multi_ADCNFTD.csv',delimiter=',')\n",
        "label = genfromtxt('/content/drive/MyDrive/labels.csv',delimiter=',') # label = genfromtxt('labels_ADCNFTD.csv',delimiter=',')\n",
        "\n",
        "#my_data1 = np.squeeze(my_data[np.where((label==1)|(label==2)),:])\n",
        "#label1 = label[np.where((label==1)|(label==2))]\n",
        "my_data1= my_data\n",
        "\n",
        "# split data in train and test (the data is randomized before )\n",
        "x_train = my_data1[0:int(len(my_data1)*0.7)]\n",
        "x_test = my_data1[int(len(my_data1)*0.7)+1:len(my_data1)]\n",
        "\n",
        "\n",
        "label_1 = label\n",
        "y_train = label_1[0:int(len(my_data1)*0.7)]\n",
        "y_test = label_1[int(len(my_data1)*0.7)+1:len(my_data1)]\n",
        "\n",
        "\n",
        "original_dim = 13225"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE8RFSt1pImR"
      },
      "source": [
        "**VAE TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7LQ_BE3__Dr"
      },
      "outputs": [],
      "source": [
        "# network parameters\n",
        "input_shape = (original_dim, )\n",
        "intermediate_dim = 1028\n",
        "batch_size = 128\n",
        "latent_dim = 2\n",
        "epochs = 10\n",
        "\n",
        "# VAE model = encoder + decoder\n",
        "# build encoder model\n",
        "inputs = Input(shape=input_shape, name='encoder_input')\n",
        "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
        "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
        "\n",
        "# use reparameterization trick to push the sampling out as input\n",
        "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
        "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "# instantiate encoder model\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "encoder.summary()\n",
        "#plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
        "\n",
        "# build decoder model\n",
        "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
        "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
        "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
        "\n",
        "# instantiate decoder model\n",
        "decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "decoder.summary()\n",
        "#plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
        "\n",
        "# instantiate VAE model\n",
        "outputs = decoder(encoder(inputs)[2])\n",
        "vae = Model(inputs, outputs, name='vae_mlp')\n",
        "\n",
        "\n",
        "\n",
        "models = (encoder, decoder)\n",
        "data = (x_test, y_test)\n",
        "reconstruction_loss = binary_crossentropy(inputs,outputs)\n",
        "\n",
        "reconstruction_loss *= original_dim\n",
        "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "kl_loss = K.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "vae.add_loss(vae_loss)\n",
        "optimizer = Adam(lr=0.004)\n",
        "vae.compile(optimizer=optimizer) # vae.compile(optimizer='adam')\n",
        "vae.summary()\n",
        "\n",
        "from keras.callbacks import History\n",
        "history = History()\n",
        "vae.fit(x_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[history])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLkMhE6EuI6S"
      },
      "source": [
        "**CLASSIFICATOR TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDrNG_U3Tc2Y"
      },
      "outputs": [],
      "source": [
        "num_classes = 5\n",
        "\n",
        "# Classification model for the latent space\n",
        "x_train_encoded,_,_ = encoder.predict(x_train, batch_size=batch_size)\n",
        "x_test_encoded,_,_ = encoder.predict(x_test, batch_size=batch_size)\n",
        "\n",
        "y_train_encoded = to_categorical(y_train, num_classes)\n",
        "y_test_encoded = to_categorical(y_test, num_classes)\n",
        "\n",
        "classifier = Sequential([Dense(64, activation='tanh', input_shape=(latent_dim,)),\n",
        "                         Dense(32, activation='tanh'), Dense(32, activation='tanh'),\n",
        "                         Dense(num_classes, activation='softmax')])\n",
        "optimizer = Adam(lr=0.001)\n",
        "classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "classifier.fit(x_train_encoded, y_train_encoded, epochs=30, batch_size=32, validation_data=(x_test_encoded,y_test_encoded))\n",
        "\n",
        "label_pred = classifier.predict(x_test_encoded)\n",
        "label_pred = np.argmax(label_pred, axis=1)\n",
        "accuracy = accuracy_score(y_test,label_pred)\n",
        "print(\"Classification accuracy: \", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2LutC8GuaP2"
      },
      "source": [
        "**x100**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ih9C7nOukkh"
      },
      "outputs": [],
      "source": [
        "num_classes = 5\n",
        "acc_class = []\n",
        "rep = list(range(1,101))\n",
        "for i in range(100):\n",
        "  # Classification model for the latent space\n",
        "  x_train_encoded,_,_ = encoder.predict(x_train, batch_size=batch_size)\n",
        "  x_test_encoded,_,_ = encoder.predict(x_test, batch_size=batch_size)\n",
        "\n",
        "  y_train_encoded = to_categorical(y_train, num_classes)\n",
        "  y_test_encoded = to_categorical(y_test, num_classes)\n",
        "\n",
        "  classifier = Sequential([Dense(64, activation='tanh', input_shape=(latent_dim,)),\n",
        "                          Dense(32, activation='tanh'), Dense(32, activation='tanh'),\n",
        "                          Dense(num_classes, activation='softmax')])\n",
        "  optimizer = Adam(lr=0.001)\n",
        "  classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  classifier.fit(x_train_encoded, y_train_encoded, epochs=30, batch_size=32, validation_data=(x_test_encoded,y_test_encoded))\n",
        "\n",
        "  label_pred = classifier.predict(x_test_encoded)\n",
        "  label_pred = np.argmax(label_pred, axis=1)\n",
        "  accuracy = accuracy_score(y_test,label_pred)\n",
        "  acc_class.append(accuracy)\n",
        "  print(\"Classification accuracy for trial \", i, ': ', accuracy)\n",
        "\n",
        "legend = ['Correct Classes']\n",
        "plt.hist([round(x,3) for x in acc_class])\n",
        "plt.xlabel(\"Acurracy\")\n",
        "plt.ylabel('#')\n",
        "plt.legend(legend)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch1suj4SxPAx"
      },
      "source": [
        "**SAVING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3SKn41Hv2CD",
        "outputId": "aa00f7b7-d01b-41e7-9d68-827e4dc676fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 0s 4ms/step\n"
          ]
        }
      ],
      "source": [
        "z_mean, _, _ = encoder.predict(x_test, batch_size=batch_size)\n",
        "\n",
        "savetxt('/content/drive/MyDrive/z_mean0_2D.csv', z_mean[:,0], delimiter=',')\n",
        "savetxt('/content/drive/MyDrive/z_mean1_2D.csv', z_mean[:,1], delimiter=',')\n",
        "savetxt('/content/drive/MyDrive/labels_dataVAE_2D.csv', y_test, delimiter=',')\n",
        "savetxt('/content/drive/MyDrive/TFG_paper/2D/accuracy.csv', acc_class, delimiter=',')\n",
        "vae.save_weights('/content/drive/MyDrive/vae_2D_Midad_Interdim.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7I8AYGE7lvl"
      },
      "source": [
        "**NULL HYPOTHESIS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UfG1BDE7lG9"
      },
      "outputs": [],
      "source": [
        "# MNIST dataset\n",
        "\n",
        "# load data\n",
        "my_data = genfromtxt('/content/drive/MyDrive/dataset_FCsim_NH.csv',delimiter=',') # my_data = genfromtxt('FC_multi_ADCNFTD.csv',delimiter=',')\n",
        "label = genfromtxt('/content/drive/MyDrive/labels_NH.csv',delimiter=',') # label = genfromtxt('labels_ADCNFTD.csv',delimiter=',')\n",
        "\n",
        "#my_data1 = np.squeeze(my_data[np.where((label==1)|(label==2)),:])\n",
        "#label1 = label[np.where((label==1)|(label==2))]\n",
        "my_data1= my_data\n",
        "\n",
        "# split data in train and test (the data is randomized before )\n",
        "x_train = my_data1[0:int(len(my_data1)*0.7)]\n",
        "x_test = my_data1[int(len(my_data1)*0.7)+1:len(my_data1)]\n",
        "\n",
        "\n",
        "label_1 = label\n",
        "y_train = label_1[0:int(len(my_data1)*0.7)]\n",
        "y_test = label_1[int(len(my_data1)*0.7)+1:len(my_data1)]\n",
        "\n",
        "\n",
        "original_dim = 13225"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-KH1X7x8AO6"
      },
      "outputs": [],
      "source": [
        "# network parameters\n",
        "input_shape = (original_dim, )\n",
        "intermediate_dim = 1028\n",
        "batch_size = 128\n",
        "latent_dim = 2\n",
        "epochs = 10\n",
        "\n",
        "# VAE model = encoder + decoder\n",
        "# build encoder model\n",
        "inputs = Input(shape=input_shape, name='encoder_input')\n",
        "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
        "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
        "\n",
        "# use reparameterization trick to push the sampling out as input\n",
        "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
        "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "# instantiate encoder model\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "encoder.summary()\n",
        "#plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
        "\n",
        "# build decoder model\n",
        "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
        "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
        "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
        "\n",
        "# instantiate decoder model\n",
        "decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "decoder.summary()\n",
        "#plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
        "\n",
        "# instantiate VAE model\n",
        "outputs = decoder(encoder(inputs)[2])\n",
        "vae = Model(inputs, outputs, name='vae_mlp')\n",
        "\n",
        "\n",
        "\n",
        "models = (encoder, decoder)\n",
        "data = (x_test, y_test)\n",
        "reconstruction_loss = binary_crossentropy(inputs,outputs)\n",
        "\n",
        "reconstruction_loss *= original_dim\n",
        "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "kl_loss = K.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "vae.add_loss(vae_loss)\n",
        "optimizer = Adam(lr=0.004)\n",
        "vae.compile(optimizer=optimizer) # vae.compile(optimizer='adam')\n",
        "vae.summary()\n",
        "\n",
        "from keras.callbacks import History\n",
        "history = History()\n",
        "vae.fit(x_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[history])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bz-my45_8H4x"
      },
      "outputs": [],
      "source": [
        "num_classes = 5\n",
        "acc_NH = []\n",
        "rep = list(range(1,101))\n",
        "for i in range(100):\n",
        "  # Classification model for the latent space\n",
        "  x_train_encoded,_,_ = encoder.predict(x_train, batch_size=batch_size)\n",
        "  x_test_encoded,_,_ = encoder.predict(x_test, batch_size=batch_size)\n",
        "\n",
        "  y_train_encoded = to_categorical(y_train, num_classes)\n",
        "  y_test_encoded = to_categorical(y_test, num_classes)\n",
        "\n",
        "  classifier = Sequential([Dense(64, activation='tanh', input_shape=(latent_dim,)),\n",
        "                          Dense(32, activation='tanh'), Dense(32, activation='tanh'),\n",
        "                          Dense(num_classes, activation='softmax')])\n",
        "  optimizer = Adam(lr=0.001)\n",
        "  classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  classifier.fit(x_train_encoded, y_train_encoded, epochs=30, batch_size=32, validation_data=(x_test_encoded,y_test_encoded))\n",
        "\n",
        "  label_pred = classifier.predict(x_test_encoded)\n",
        "  label_pred = np.argmax(label_pred, axis=1)\n",
        "  accuracy = accuracy_score(y_test,label_pred)\n",
        "  acc_NH.append(accuracy)\n",
        "  print(\"Classification accuracy for trial \", i, ': ', accuracy)\n",
        "\n",
        "legend = ['Random Classes']\n",
        "plt.hist([round(x,3) for x in acc_NH], color= 'maroon')\n",
        "plt.xlabel(\"Acurracy\")\n",
        "plt.ylabel('#')\n",
        "plt.legend(legend)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kweP8Yna8Ve0"
      },
      "outputs": [],
      "source": [
        "z_mean, _, _ = encoder.predict(x_test, batch_size=batch_size)\n",
        "\n",
        "savetxt('/content/drive/MyDrive/TFG_paper/12D/accuracy_NH.csv', acc_NH, delimiter=',')\n",
        "#vae.save_weights('/content/drive/MyDrive/TFG_psychosis/Coses_finals/12D/NH/vae_12D_Midad_Interdim.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGz4W0tjjdty"
      },
      "outputs": [],
      "source": [
        "legend = ['Correct Classes', 'Random Classes']\n",
        "plt.hist([round(x,3) for x in acc_class])\n",
        "plt.hist([round(x,3) for x in acc_NH], color= 'maroon')\n",
        "plt.xlabel(\"Acurracy\")\n",
        "plt.ylabel('#')\n",
        "plt.legend(legend)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}